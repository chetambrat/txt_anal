"""

 1) спроектировать модуль хранения и чтения словарей.
 2) на английском или на русском собрать 3 корпуса текста (тематическая выборка – 2 штуки (выбрать две тематики на одном
  языке и собрать 20-30 текстов (не твиты, но при этом не глава «Война и мир», что-то среднее, размерности статьи на
   хабре) по каждой тематике + «просто» тексты привести к одному формату).
 3) спроектировать словарь стоп слов (три выборки сливаем в один документ и рассчитываем TF-IDF для каждого слова,
  отфильтровать и посмотреть, что получилось).
 4) для первых двух выборок (тематических) контрастным методом найти специфические термины, характерные для данных
  выборок (выбрасываем стоп-слова и смотрим слова, которые имеют высокий вес для одной выборки и низкий вес для другой
   выборки). Можно составить словарь специфических терминов.
"""

import nltk
import os
from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize


def import_raw_text(folder):
    result_list = []
    file_container = [raw_file for raw_file in os.walk(folder)]
    file_container = file_container[0][2]
    for files in file_container:
        with open(f"{folder}/{files}", "r", encoding="UTF-8") as f:
            result_list.append(f.read())

    return result_list


print(import_raw_text("./CCP"))